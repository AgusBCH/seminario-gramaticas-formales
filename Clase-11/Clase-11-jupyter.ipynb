{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsers de dependencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requerimientos\n",
    "- Python 3\n",
    "- Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "from nltk import Tree\n",
    "from spacy import displacy \n",
    "from nltk.parse import malt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No poseen distinción entre símbolos no terminales y terminales. Las estructuras representan relaciones de dependencia entre terminales.\n",
    "Ejemplos de parsers de dependencias:\n",
    "* Projective Dependency Parser de NLTK (https://www.nltk.org/_modules/nltk/parse/projectivedependencyparser.html)\n",
    "* Maltparser (http://www.maltparser.org/)\n",
    "* SyntaxNet (Estaba alojado en https://opensource.google.com/projects/syntaxnet, como parte de los recursos de la librería para Inteligencia Artificial TensorFlow de Google, pero en este momento no está disponible y se [rumorea](https://github.com/tensorflow/models/issues/8411) que se lo va a mover al github de [google-research](https://github.com/google-research/google-research))\n",
    "* Dependency parser de Spacy (https://spacy.io/usage/linguistic-features#dependency-parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projective Dependency Parser NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dep_parser(sentence, grammar):                   # define una función llamada dep_parser con dos argumentos\n",
    "    sentence = sentence.lower()                     # convierte a minúscula la oración\n",
    "    if sentence.endswith('.'):                      # si la oración termina con un punto\n",
    "        sent = re.sub('\\.',' ',sentence)            # se lo quita\n",
    "    else:                                           # si no\n",
    "        sent = sentence                             # la toma como está\n",
    "    sent = sent.split()                             # divide la oración en palabras\n",
    "    dep_gram = nltk.data.load(grammar)              # carga la gramática a nltk\n",
    "    dep_gram = nltk.DependencyGrammar.fromstring(dep_gram) # parsea la gramática como gramática de dependencias\n",
    "    pdp = nltk.ProjectiveDependencyParser(dep_gram) # Carga la gramática en el parser\n",
    "    print(dep_gram)                                  # imprime mi gramática\n",
    "    for tree in pdp.parse(sent):              # para cada árbol posible en mi gramática para esa oración\n",
    "        print(tree)                                 # lo imprimo\n",
    "        return(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para correr el Proyective Dependency Parser\n",
    "\n",
    "oracion1 = 'Pablo explotó el globo'  # Define la oración a analizar\n",
    "grammar = 'gramaticas/DG1.txt'       # establece cuál va a ser mi gramática\n",
    "dep_parser(oracion1, grammar)        # Para correr la función"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Projective Dependency Parser NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npdep_parser(sentence, grammar):                   # define una función llamada dep_parser con dos argumentos\n",
    "    sentence = sentence.lower()                     # convierte a minúscula la oración\n",
    "    if sentence.endswith('.'):                      # si la oración termina con un punto\n",
    "        sent = re.sub('\\.',' ',sentence)            # se lo quita\n",
    "    else:                                           # si no\n",
    "        sent = sentence                             # la toma como está\n",
    "    sent = sent.split()                             # divide la oración en palabras\n",
    "    dep_gram = nltk.data.load(grammar)              # carga la gramática a nltk\n",
    "    dep_gram = nltk.DependencyGrammar.fromstring(dep_gram) # parsea la gramática como gramática de dependencias\n",
    "    pdp = nltk.NonprojectiveDependencyParser(dep_gram) # Carga la gramática en el parser\n",
    "    print(sent)\n",
    "    print(dep_gram)                                  # imprime mi gramática\n",
    "    g, = pdp.parse(sent)\n",
    "    print(g.root['word'])\n",
    "    print(g.tree())\n",
    "    #    for tree in pdp.parse(sent):              # para cada árbol posible en mi gramática para esa oración\n",
    "#        print(tree)                                 # lo imprimo\n",
    "#        return(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para correr el Nonproyective Dependency Parser\n",
    "\n",
    "oracion1 = 'Pablo explotó el globo'  # Define la oración a analizar\n",
    "grammar = 'gramaticas/DG1.txt'       # establece cuál va a ser mi gramática\n",
    "npdep_parser(oracion1, grammar)        # Para correr la función"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy - Dependency parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nota para quien no tenga la MV: \n",
    "\n",
    "Antes de correr hay que instalar spacy. Con pip3, eso se puede hacer con el comando \n",
    "\n",
    "`pip3 install spacy`\n",
    "\n",
    "Hay que instalar también es_core_news_sm, un modelo entrenado mediante un corpus del español, con el comando\n",
    "\n",
    "`python3 -m spacy download es_core_news_sm`\n",
    "\n",
    "Alternativamente puede probarse de instalar es_core_news_md.\n",
    "\n",
    "`python3 -m spacy download es_core_news_md`\n",
    "\n",
    "En ese caso, para correrlo hay que cambiar en el código de abajo `es_core_news_sm` por `es_core_news_md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gramaticadependencias(sentence):       #Define la función\n",
    "    nlp = spacy.load('es_core_news_sm')    #Carga el modelo entrenado\n",
    "    doc = nlp(sentence)                    #define una variable doc con la oración procesada por el modelo\n",
    "    #for token in doc:               \n",
    "        #print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "        #    [child for child in token.children])\n",
    "    displacy.render(doc, style='dep', jupyter=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Escribí una oración')\n",
    "oracion5 = input()\n",
    "gramaticadependencias(oracion5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Malt Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrucciones\n",
    "\n",
    "Crear la carpeta malt y adentro bajar los siguientes archivos:\n",
    "- Malt Parser de [http://www.maltparser.org/download.html](http://www.maltparser.org/download.html)\n",
    "- Bajar el modelo entrenado engmalt.poly-1.7 de [http://www.maltparser.org/mco/english_parser/engmalt.html](http://www.maltparser.org/mco/english_parser/engmalt.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maltparser(sentence, parserversion, langmodel):\n",
    "    mp = malt.MaltParser(paserversion, langmodel)\n",
    "    stemma = mp.parse_one(sentence.split()).tree()\n",
    "    print(stemma)\n",
    "    return(stemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parserversion = 'malt/maltparser-1.9.2' # Define la versión del parser. \n",
    "langmodel = 'malt/engmalt.poly-1.7.mco'# Define el modelo entrenado poly\n",
    "#lamgmodel = 'engmalt.linear-1.7.mco' # Defuine el modelo entrenado linear\n",
    "print('Escribí una oración')\n",
    "oracion8 = input()\n",
    "maltparser(oracion8, parserversion, langmodel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
