{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsers de dependencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requerimientos\n",
    "- Python 3\n",
    "- Spacy\n",
    "- NLTK\n",
    "- MaltParser\n",
    "- Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "from nltk import Tree\n",
    "from spacy import displacy \n",
    "from nltk.parse import malt\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No poseen distinción entre símbolos no terminales y terminales. Las estructuras representan relaciones de dependencia entre terminales.\n",
    "Ejemplos de parsers de dependencias:\n",
    "* Projective Dependency Parser de NLTK (https://www.nltk.org/_modules/nltk/parse/projectivedependencyparser.html)\n",
    "* Maltparser (http://www.maltparser.org/)\n",
    "* SyntaxNet (Estaba alojado en https://opensource.google.com/projects/syntaxnet, como parte de los recursos de la librería para Inteligencia Artificial TensorFlow de Google, pero en este momento no está disponible y se [rumorea](https://github.com/tensorflow/models/issues/8411) que se lo va a mover al github de [google-research](https://github.com/google-research/google-research))\n",
    "* Dependency parser de Spacy (https://spacy.io/usage/linguistic-features#dependency-parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projective Dependency Parser NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dep_parser(sentence, grammar):                   # define una función llamada dep_parser con dos argumentos\n",
    "    sentence = sentence.lower()                     # convierte a minúscula la oración\n",
    "    if sentence.endswith('.'):                      # si la oración termina con un punto\n",
    "        sent = re.sub('\\.',' ',sentence)            # se lo quita\n",
    "    else:                                           # si no\n",
    "        sent = sentence                             # la toma como está\n",
    "    sent = sent.split()                             # divide la oración en palabras\n",
    "    dep_gram = nltk.data.load(grammar)              # carga la gramática a nltk\n",
    "    dep_gram = nltk.DependencyGrammar.fromstring(dep_gram) # parsea la gramática como gramática de dependencias\n",
    "    pdp = nltk.ProjectiveDependencyParser(dep_gram) # Carga la gramática en el parser\n",
    "    print(dep_gram)                                  # imprime mi gramática\n",
    "    for tree in pdp.parse(sent):              # para cada árbol posible en mi gramática para esa oración\n",
    "        print(tree)                                 # lo imprimo\n",
    "        return(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para correr el Proyective Dependency Parser\n",
    "\n",
    "oracion1 = 'Pablo explotó el globo'  # Define la oración a analizar\n",
    "grammar = 'gramaticas/DG1.txt'       # establece cuál va a ser mi gramática\n",
    "dep_parser(oracion1, grammar)        # Para correr la función"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Projective Dependency Parser NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npdep_parser(sentence, grammar):                   # define una función llamada dep_parser con dos argumentos\n",
    "    sentence = sentence.lower()                     # convierte a minúscula la oración\n",
    "    if sentence.endswith('.'):                      # si la oración termina con un punto\n",
    "        sent = re.sub('\\.',' ',sentence)            # se lo quita\n",
    "    else:                                           # si no\n",
    "        sent = sentence                             # la toma como está\n",
    "    sent = sent.split()                             # divide la oración en palabras\n",
    "    dep_gram = nltk.data.load(grammar)              # carga la gramática a nltk\n",
    "    dep_gram = nltk.DependencyGrammar.fromstring(dep_gram) # parsea la gramática como gramática de dependencias\n",
    "    pdp = nltk.NonprojectiveDependencyParser(dep_gram) # Carga la gramática en el parser\n",
    "    print(sent)\n",
    "    print(dep_gram)                                  # imprime mi gramática\n",
    "    g, = pdp.parse(sent)\n",
    "    print(g.root['word'])\n",
    "    structure = g.tree()\n",
    "    print(structure)\n",
    "    return(structure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para correr el Nonproyective Dependency Parser\n",
    "\n",
    "oracion1 = 'fede fuma el cigarrillo'  # Define la oración a analizar\n",
    "grammar1 = 'gramaticas/DG1.txt'       # establece cuál va a ser mi gramática\n",
    "npdep_parser(oracion1, grammar1)        # Para correr la función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para correr el Nonproyective Dependency Parser\n",
    "\n",
    "oracion2 = 'quién fuma el cigarrillo'  # Define la oración a analizar\n",
    "#oracion2 = 'quién dijo fede que fuma'  # Define la oración a analizar\n",
    "#oracion2 = 'qué dijo fede que fuma'  # Define la oración a analizar\n",
    "# Habría que arreglar la función npdep_parser para que pueda tomar estas dos últimas oraciones\n",
    "grammar2 = 'gramaticas/DG2.txt'       # establece cuál va a ser mi gramática\n",
    "npdep_parser(oracion2, grammar2)        # Para correr la función"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy - Dependency parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nota para quien no tenga la MV: \n",
    "\n",
    "Antes de correr hay que instalar spacy. Con pip3, eso se puede hacer con el comando \n",
    "\n",
    "`pip3 install spacy`\n",
    "\n",
    "Hay que instalar también es_core_news_sm, un modelo entrenado mediante un corpus del español, con el comando\n",
    "\n",
    "`python3 -m spacy download es_core_news_sm`\n",
    "\n",
    "Alternativamente puede probarse de instalar es_core_news_md.\n",
    "\n",
    "`python3 -m spacy download es_core_news_md`\n",
    "\n",
    "En ese caso, para correrlo hay que cambiar en el código de abajo `es_core_news_sm` por `es_core_news_md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gramaticadependencias(sentence):       #Define la función\n",
    "    nlp = spacy.load('es_core_news_sm')    #Carga el modelo entrenado\n",
    "    doc = nlp(sentence)                    #define una variable doc con la oración procesada por el modelo\n",
    "    #for token in doc:               \n",
    "        #print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "        #    [child for child in token.children])\n",
    "    displacy.render(doc, style='dep', jupyter=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Escribí una oración')\n",
    "oracion5 = input()\n",
    "gramaticadependencias(oracion5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Malt Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrucciones\n",
    "\n",
    "Crear la carpeta malt y adentro bajar los siguientes archivos:\n",
    "- Malt Parser de [http://www.maltparser.org/download.html](http://www.maltparser.org/download.html)\n",
    "- Bajar el modelo entrenado engmalt.poly-1.7 de [http://www.maltparser.org/mco/english_parser/engmalt.html](http://www.maltparser.org/mco/english_parser/engmalt.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['MALT_PARSER']=\"/home/grmf/Escritorio/seminario-gramaticas-formales/Clase-11/malt/maltparser-1.9.2/\"\n",
    "os.environ['MALT_MODEL']=\"/home/grmf/Escritorio/seminario-gramaticas-formales/Clase-11/malt/maltparser-1.9.2/engmalt.poly-1.7.mco\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parserversion = '/home/grmf/Escritorio/seminario-gramaticas-formales/Clase-11/malt/maltparser-1.9.2/' # Define la versión del parser. \n",
    "langmodel = '/home/grmf/Escritorio/seminario-gramaticas-formales/Clase-11/malt/maltparser-1.9.2/engmalt.poly-1.7.mco'# Define el modelo entrenado poly\n",
    "#lamgmodel = 'engmalt.linear-1.7.mco' # Define el modelo entrenado linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maltParser = nltk.parse.malt.MaltParser(parserversion, langmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracion8 = input()\n",
    "stemma = maltParser.parse_one(oracion8.split()).tree()\n",
    "print(stemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PystanfordDependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El parser de Stanford se mudó a un nuevo repositorio y cambió su nombre a Stanza. Se puede encontrar la documentación en [https://stanfordnlp.github.io/stanza/](https://stanfordnlp.github.io/stanza/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza.download('es') # Baja el modelo para el español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline('es') # initialize Spanish neural pipeline\n",
    "doc = nlp(\"ella durmió al calor de las masas.\") # run annotation over a sentence\n",
    "print(doc)\n",
    "print(doc.entities)\n",
    "a = doc.sentences[0]\n",
    "a.print_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
