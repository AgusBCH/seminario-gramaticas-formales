{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de correr esta notebook hay que instalar Python 3, nltk para Python 3 y matplotlib para Python 3. cuidado al instalar, porque si tienen Python 2 en la computadora, instala todo por defecto para esa versión y entonces no funciona para la 3. Si se usa pip para instalar, usar el comando pip3. Pueden encontrar información sobre instalación en la página del Grupo de Lingüística Computacional Para que algunos comandos funcionen, además de hacer las instalaciones correspondientes, hay que descargar algunos archivos. En todos los casos, el código está armado para que esos archivos se descarguen en la misma carpeta en la que está la jupyter notebook. En caso de que se utilice otra ubicación, hay que editar el código."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gramáticas \n",
    "\n",
    "Las gramáticas se expresan usualmente en forma de reglas de reescritura de la forma X -> Z (X se reescribe como Z), aunque estas  reglas pueden concebirse también como árboles parcialmente construidos o como restricciones combinatorias. Las gramáticas típicamente permiten construir grafos dirigidos, coloquialmente denominados ''árboles''.\n",
    "Existen tres grandes formas de construir las gramáticas según cómo se conciba la representación de la estructura jerárquica:\n",
    "* Gramáticas basadas en constituyentes\n",
    "* Gramáticas basadas en dependencias\n",
    "* Gramáticas categoriales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gramática basada en constituyentes](constituyentes.png)\n",
    "![Gramática basada en dependencias](dependencias.png)\n",
    "![Gramática categorial](categorial.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gramáticas basadas en constituyentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poseen distinción entre nodos terminales y nodos no terminales, que especifican la categoría a la que pertenece una determinada subcadena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import re \n",
    "import os, sys\n",
    "import matplotlib "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive descent Parser\n",
    "\n",
    "- **Top-down**\n",
    "- Parte del símbolo de inicio y aplica las reglas para obtener los constituyentes inmediatos y armar el árbol hasta llegar a los símbolos terminales. Chequea coincidencia con la secuencia del input. Si no hay coincidencia, tiene que retroceder y buscar diferentes alternativas de parseo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Descent Parser\n",
    "\n",
    "def rd_parser(sentence, grammar):                   # define una función llamada rd_parser con dos argumentos\n",
    "    print(grammar)                                  # imprime mi gramática\n",
    "    sentence = sentence.lower()                     # convierte a minúscula la oración\n",
    "    if sentence.endswith('.'):                      # si la oración termina con un punto\n",
    "        sent = re.sub('\\.',' ',sentence)            # se lo quita\n",
    "    else:                                           # si no\n",
    "        sent = sentence                             # la toma como está\n",
    "    sent = sent.split()                             # divide la oración en palabras\n",
    "    rd_parser = nltk.RecursiveDescentParser(grammar) # proceso esas palabras\n",
    "    for tree in rd_parser.parse(sent):              # para cada árbol posible en mi gramática para esa oración\n",
    "        print(tree)                                 # lo imprimo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Para correr el Recursive Descent Parser\n",
    "\n",
    "print('Escribí una oración:')                          #Para que me pida que escriba una oración\n",
    "oracion1 = input()                                     #Para que me abra un campo en el que escriba la oración\n",
    "grammar = nltk.data.load('gramaticas/ContextFreeGrammar.cfg')     # establece cuál va a ser mi gramática\n",
    "rd_parser(oracion1, grammar)                           #Para correr la función"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demo del Right Descent Parser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /home/carranza/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/carranza/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/carranza/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /home/carranza/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/carranza/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/carranza/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/carranza/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/carranza/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "nltk.app.rdparser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shift Reduce Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shift Reduce Parser\n",
    "\n",
    "def sr_parser(sentence, grammar):                      # define la función sr_parser con dos argumentos\n",
    "    print(grammar)                                     # imprimte la gramática\n",
    "    sentence = sentence.lower()                        # convierte a minúscula\n",
    "    if sentence.endswith('.'):                         # si la oración termina con un punto\n",
    "        sent = re.sub('\\.',' ',sentence)               # se lo quita\n",
    "    else:                                              # si no\n",
    "        sent = sentence                                # la toma como está\n",
    "    sent = sent.split()                                # divide la oración en palabras\n",
    "    sr_parser = nltk.ShiftReduceParser(grammar)        # proceso esas palabras\n",
    "    for tree in sr_parser.parse(sent):                 # para cada árbol posible en mi gramática para esa oración\n",
    "        print(tree)                                    # lo imprimo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Escribí una oración:')                           # imprime un mensaje pidiendo que escriba una oración\n",
    "oracion2 = input()                                      # asigna a una variable mi oración como valor\n",
    "grammar = nltk.data.load('gramaticas/ContextFreeGrammar.cfg')      # asigna a una variable mi gramática como valor\n",
    "sr_parser(oracion2, grammar)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Desventajas:**\n",
    "\n",
    "- La recursividad a la izquierda (NP -> NP PP) lo lleva al loop infinito\n",
    "\n",
    "- Pierde mucho tiempo considerando las estructuras que no se corresponden con el input\n",
    "\n",
    "- En el proceso de backtracking se descartan los parseos anteriores y tiene que volver a construirlos \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demo del Shift and Reduce parser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: VP -> V NP PP will never be used\n"
     ]
    }
   ],
   "source": [
    "nltk.app.srparser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chart Parser (solo demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grammar= (\n",
      "('    ', 'S -> NP VP,')\n",
      "('    ', 'VP -> VP PP,')\n",
      "('    ', 'VP -> V NP,')\n",
      "('    ', 'VP -> V,')\n",
      "('    ', 'NP -> Det N,')\n",
      "('    ', 'NP -> NP PP,')\n",
      "('    ', 'PP -> P NP,')\n",
      "('    ', \"NP -> 'John',\")\n",
      "('    ', \"NP -> 'I',\")\n",
      "('    ', \"Det -> 'the',\")\n",
      "('    ', \"Det -> 'my',\")\n",
      "('    ', \"Det -> 'a',\")\n",
      "('    ', \"N -> 'dog',\")\n",
      "('    ', \"N -> 'cookie',\")\n",
      "('    ', \"N -> 'table',\")\n",
      "('    ', \"N -> 'cake',\")\n",
      "('    ', \"N -> 'fork',\")\n",
      "('    ', \"V -> 'ate',\")\n",
      "('    ', \"V -> 'saw',\")\n",
      "('    ', \"P -> 'on',\")\n",
      "('    ', \"P -> 'under',\")\n",
      "('    ', \"P -> 'with',\")\n",
      ")\n",
      "tokens = ['John', 'ate', 'the', 'cake', 'on', 'the', 'table']\n",
      "Calling \"ChartParserApp(grammar, tokens)\"...\n",
      "[('under',)]\n",
      "[('with',)]\n",
      "[('on',)]\n",
      "[('under',), ('with',)]\n",
      "[('ate',)]\n",
      "[('saw',)]\n",
      "[('cake',)]\n",
      "[('fork',)]\n",
      "[('table',)]\n",
      "[('cake',), ('fork',)]\n",
      "[('cookie',)]\n",
      "[('table',), ('cake',), ('fork',)]\n",
      "[('dog',)]\n",
      "[('cookie',), ('table',), ('cake',), ('fork',)]\n",
      "[('my',)]\n",
      "[('a',)]\n",
      "[('the',)]\n",
      "[('my',), ('a',)]\n",
      "[('John',)]\n",
      "[('I',)]\n",
      "[(Det, N)]\n",
      "[(NP, PP)]\n",
      "[(V, NP)]\n",
      "[(V,)]\n",
      "[(VP, PP)]\n",
      "[(V, NP), (V,)]\n",
      "S [(NP, VP)]\n",
      "VP [(VP, PP), (V, NP), (V,)]\n",
      "NP [(Det, N), (NP, PP)]\n",
      "PP [(P, NP)]\n",
      "NP [('John',), ('I',)]\n",
      "Det [('the',), ('my',), ('a',)]\n",
      "N [('dog',), ('cookie',), ('table',), ('cake',), ('fork',)]\n",
      "V [('ate',), ('saw',)]\n",
      "P [('on',), ('under',), ('with',)]\n"
     ]
    }
   ],
   "source": [
    "#Demo para el Chart Parser\n",
    "nltk.app.chartparser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bllip Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes hay que instalar el bllip parser. \n",
    "\n",
    "Para hacerlo, correr el siguiente comando en la terminal:\n",
    "\n",
    "    pip3 install --user bllipparser\n",
    "\n",
    "Brown Laboratory for Linguistic Information Processing\n",
    "\n",
    "Introduce gramáticas a partir de un corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bllipparser import RerankingParser                             #Importa el parser\n",
    "from bllipparser.ModelFetcher import download_and_install_model     # Descarga e instala el \"modelo\"\n",
    "\n",
    "model_dir = download_and_install_model('WSJ', 'tmp/models')         #Crea una variable con el \"modelo\"\n",
    "rrp = RerankingParser.from_unified_model_dir(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(S1 (S (NP (NNP john)) (VP (VBZ runs) (PP (IN through) (NP (DT the) (NN hill))))))'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oracion2 = \"john runs through the hill\"\n",
    "rrp.simple_parse(oracion2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(S1 (S (S (NP (DT No) (NN one)) (VP (VBD saw) (S (NP (PRP him)) (VP (VBP disembark) (PP (IN in) (NP (DT the) (JJ unanimous) (NN night))))))) (, ,) (S (NP (DT no) (NN one)) (VP (VBD saw) (S (NP (DT the) (NN bamboo) (NN canoe)) (VP (VB sink) (PP (IN into) (NP (DT the) (JJ sacred) (NN mud))))))) (, ,) (CC but) (S (PP (IN in) (NP (DT a) (JJ few) (NNS days))) (NP (EX there)) (VP (VBD was) (NP (NP (DT no) (NN one)) (SBAR (WHNP (WP who)) (S (VP (VBD did) (RB not) (VP (VB know) (SBAR (IN that) (S (NP (DT the) (JJ taciturn) (NN man)) (VP (VBD came) (PP (IN from) (NP (DT the) (NNP South)))))))))))))))'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oracion3 = \"No one saw him disembark in the unanimous night, no one saw the bamboo canoe sink into the sacred mud, but in a few days there was no one who did not know that the taciturn man came from the South\"\n",
    "rrp.simple_parse(oracion3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Escribí una oración en inglés')\n",
    "oracion4 = input()\n",
    "rrp.simple_parse(oracion4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gramáticas basadas en dependencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No poseen distinción entre símbolos no terminales y terminales. Las estructuras representan relaciones de dependencia entre terminales.\n",
    "Ejemplos de parsers de dependencias:\n",
    "* Maltparser (http://www.maltparser.org/)\n",
    "* SyntaxNet (Estaba alojado en https://opensource.google.com/projects/syntaxnet, como parte de los recursos de la librería para Inteligencia Artificial TensorFlow de Google, pero en este momento no está disponible y se [rumorea](https://github.com/tensorflow/models/issues/8411) que se lo va a mover al github de [google-research](https://github.com/google-research/google-research))\n",
    "* Dependency parser de Spacy (https://spacy.io/usage/linguistic-features#dependency-parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy - Dependency parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nota para quien no tenga la MV: \n",
    "\n",
    "Antes de correr hay que instalar spacy. Con pip3, eso se puede hacer con el comando \n",
    "\n",
    "`pip3 install spacy`\n",
    "\n",
    "Hay que instalar también es_core_news_sm, un modelo entrenado mediante un corpus del español, con el comando\n",
    "\n",
    "`python3 -m spacy download es_core_news_sm`\n",
    "\n",
    "Alternativamente puede probarse de instalar es_core_news_md.\n",
    "\n",
    "`python3 -m spacy download es_core_news_md`\n",
    "\n",
    "En ese caso, para correrlo hay que cambiar en el código de abajo `es_core_news_sm` por `es_core_news_md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk import Tree\n",
    "from spacy import displacy \n",
    "\n",
    "def gramaticadependencias(sentence):       #Define la función\n",
    "    nlp = spacy.load('es_core_news_sm')    #Carga el modelo entrenado\n",
    "    doc = nlp(sentence)                    #define una variable doc con la oración procesada por el modelo\n",
    "    #for token in doc:               \n",
    "        #print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "        #    [child for child in token.children])\n",
    "    displacy.render(doc, style='dep', jupyter=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escribí una oración\n",
      "Fernando nos aprueba a todos.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"es\" id=\"22808df49f274e349ff10735f4ae04a9-0\" class=\"displacy\" width=\"925\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Fernando</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">nos</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">aprueba</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">todos.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-22808df49f274e349ff10735f4ae04a9-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,2.0 400.0,2.0 400.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-22808df49f274e349ff10735f4ae04a9-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-22808df49f274e349ff10735f4ae04a9-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-22808df49f274e349ff10735f4ae04a9-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-22808df49f274e349ff10735f4ae04a9-0-2\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-22808df49f274e349ff10735f4ae04a9-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,179.0 L587,167.0 603,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-22808df49f274e349ff10735f4ae04a9-0-3\" stroke-width=\"2px\" d=\"M420,177.0 C420,2.0 750.0,2.0 750.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-22808df49f274e349ff10735f4ae04a9-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,179.0 L758.0,167.0 742.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Escribí una oración')\n",
    "oracion5 = input()\n",
    "gramaticadependencias(oracion5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gramáticas Categoriales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las gramáticas categoriales están conformadas principalmente por un conjunto reducido de reglas y un léxico sumamente rico.\n",
    "Las reglas que utiliza OpenCCG, que es el parser categorial que vamos a ver son las siguientes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![reglas categoriales](reglascategorialesopenccg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construir una gramática categorial consiste principalmente en elaborar un léxico lo suficientemente rico, ya que las gramáticas categoriales son fuertemente lexicalistas. En ellas, la categoría a la que pertenece cada entrada léxica codifica sus posibilidades combinatorias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinatory Categorial Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combinatory Categorial Grammar\n",
    "\n",
    "from nltk.ccg import chart, lexicon\n",
    "\n",
    "def combinatory_parser(sentence):   \n",
    "    sentence = sentence.lower()                                     # convierte a minúscula\n",
    "    if sentence.endswith('.'):                                      # si la oración termina con un punto\n",
    "        sent = re.sub('\\.',' ',sentence)                            # se lo quita\n",
    "    else:                                                           # si no\n",
    "        sent = sentence                                             # la toma como está\n",
    "    sent = sent.split()                                             # divide la oración en palabras\n",
    "    archivo = open('gramaticas/CategorialGrammar2.txt', 'r')\n",
    "    codigogram = archivo.read()\n",
    "    lex = lexicon.fromstring(codigogram)\n",
    "    print(lex)\n",
    "    parser = chart.CCGChartParser(lex, chart.DefaultRuleSet)\n",
    "    archivo.close()\n",
    "    for parse in parser.parse(sent):  # doctest: +SKIP\n",
    "         chart.printCCGDerivation(parse)\n",
    "         #break       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Escribí una oración')\n",
    "oracion5 = input()\n",
    "combinatory_parser(oracion5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gramáticas basadas en rasgos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las gramáticas se pueden enriquecer con el uso de rasgos. Los rasgos son pares de atributo valor. En una gramática con rasgos, los rasgos se heredan de las entradas léxicas a los nodos superiores. Las reglas especifican los rasgos que sus nodos hijos deben compartir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% start S\n",
      "#Adaptado al español de la gramática elaborada por Klein para el libro de NLTK\n",
      "#\n",
      "# ###################\n",
      "# Reglas de la Gramática\n",
      "# ###################\n",
      "# Reescritura de la Raíz\n",
      "S -> NP[NUM=?n] VP[NUM=?n]\n",
      "# Reescritura de NP\n",
      "NP[NUM=?n] -> PropN[NUM=?n] \n",
      "NP[NUM=?n,GEN=?g] -> Det[NUM=?n,GEN=?g] N[NUM=?n,GEN=?g]\n",
      "# Reescritura de VP\n",
      "VP[TENSE=?t, NUM=?n] -> V[TENSE=?t, NUM=?n]\n",
      "# ###################\n",
      "# Lexical Productions\n",
      "# ###################\n",
      "Det[NUM=sg,GEN=masc] -> 'este' | 'el'\n",
      "Det[NUM=pl,GEN=masc] -> 'estos' | 'los'\n",
      "Det[NUM=sg,GEN=fem] -> 'esta' | 'la'\n",
      "Det[NUM=pl,GEN=fem] -> 'estas' | 'las'\n",
      "PropN[NUM=sg]-> 'Cata' | 'Julia' | 'Fede' | 'Fer' | 'Martín' | 'Maca' | 'Vicky' | 'Pablo'\n",
      "N[NUM=sg,GEN=fem] -> 'chica' | 'mujer' | 'persona' | 'criatura'\n",
      "N[NUM=sg,GEN=masc] -> 'chico' | 'hombre' | 'sujeto' \n",
      "N[NUM=pl,GEN=fem] -> 'chicas' | 'mujeres' | 'personas' | 'criaturas'\n",
      "N[NUM=pl,GEN=masc] -> 'chicos' | 'hombres' | 'sujetos' \n",
      "V[TENSE=pres,NUM=sg] -> 'desaparece' | 'camina' | 'muerde' | 'llora' | 'aparece' | 'viene' | 'estornudan'\n",
      "V[TENSE=pres,NUM=pl] -> 'desaparecen' | 'caminan' | 'lloran' | 'muerden' | 'aparecen' | 'vienen' | 'estornudan'\n",
      "V[TENSE=pas,NUM=sg] -> 'desapareció' | 'caminó' | 'mordió' | 'lloraba' | 'apareció' | 'vino' | 'estornudó'\n",
      "V[TENSE=pas,NUM=pl] -> 'desaparecieron' | 'caminaron' | 'mordieron' | 'lloraban' | 'aparecieron' | 'vinieron' | 'estornudaron'\n"
     ]
    }
   ],
   "source": [
    "nltk.data.show_cfg('gramaticas/GramaticaDeRasgos.fcfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "los chicas caminan\n",
      "<class 'str'>\n",
      "['los', 'chicas', 'caminan']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "sentence = 'los chicas caminan'\n",
    "tokens = sentence.split()\n",
    "print(sentence)\n",
    "print(type(sentence))\n",
    "print(tokens)\n",
    "print(type(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|.los .chic.cami.|\n",
      "Leaf Init Rule:\n",
      "|[----]    .    .| [0:1] 'los'\n",
      "|.    [----]    .| [1:2] 'chicas'\n",
      "|.    .    [----]| [2:3] 'caminan'\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[----]    .    .| [0:1] Det[GEN='masc', NUM='pl'] -> 'los' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[---->    .    .| [0:1] NP[GEN=?g, NUM=?n] -> Det[GEN=?g, NUM=?n] * N[GEN=?g, NUM=?n] {?g: 'masc', ?n: 'pl'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    [----]    .| [1:2] N[GEN='fem', NUM='pl'] -> 'chicas' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    .    [----]| [2:3] V[NUM='pl', TENSE='pres'] -> 'caminan' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    .    [----]| [2:3] VP[NUM='pl', TENSE='pres'] -> V[NUM='pl', TENSE='pres'] *\n"
     ]
    }
   ],
   "source": [
    "from nltk import load_parser\n",
    "cp = load_parser('gramaticas/GramaticaDeRasgos.fcfg', trace=2)\n",
    "for tree in cp.parse(tokens):\n",
    "     print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver una aplicación de las gramáticas de rasgos para dar cuenta de la intepretación semántica, ver carpeta de semántica de este mismo repositorio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gramática con slash y rasgo subcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% start S\n",
      "# Gramática para ilustrar rasgo SUBCAT y la categoría SLASH\n",
      "#\n",
      "# ###################\n",
      "# Reglas de la Gramática\n",
      "# ###################\n",
      "# Reescritura de la Raíz\n",
      "S -> NP[NUM=?n] VP[NUM=?n]\n",
      "S -> Wh[NUM=?n] VP/Wh[NUM=?n]\n",
      "# Reescritura de NP\n",
      "NP[NUM=?n] -> PropN[NUM=?n] \n",
      "NP[NUM=?n,GEN=?g] -> Det[NUM=?n,GEN=?g] N[NUM=?n,GEN=?g]\n",
      "# Reescritura de VP\n",
      "VP[NUM=?n] -> V[SUBCAT='intrans', TENSE=?t, NUM=?n]\n",
      "VP[NUM=?n] -> V[SUBCAT='decir', TENSE=?t, NUM=?n] CP\n",
      "VP/?x[NUM=?n] -> V[SUBCAT='decir', TENSE=?t, NUM=?m] NP[NUM=?m] CP/?x[NUM=?n]\n",
      "# Reescritura de CP\n",
      "CP -> C IP\n",
      "CP/?x[NUM=?n] -> C IP/?x[NUM=?n]\n",
      "# Reescritura de C\n",
      "C -> 'que'\n",
      "# Reescritura de IP\n",
      "IP -> NP[NUM=?n] VP[NUM=?n]\n",
      "IP/?x[NUM=?n] -> N/?x[NUM=?n] VP[NUM=?n]\n",
      "# ###################\n",
      "# Lexical Productions\n",
      "# ###################\n",
      "# Reescritura de determinativos\n",
      "Det[NUM=sg,GEN=masc] -> 'este' | 'el'\n",
      "Det[NUM=pl,GEN=masc] -> 'estos' | 'los'\n",
      "Det[NUM=sg,GEN=fem] -> 'esta' | 'la'\n",
      "Det[NUM=pl,GEN=fem] -> 'estas' | 'las'\n",
      "# Reescritura de Nombres propios\n",
      "PropN[NUM=sg]-> 'Cata' | 'Julia' | 'Fede' | 'Fer' | 'Martín' | 'Maca' | 'Vicky' | 'Pablo'\n",
      "# Reescritura de N\n",
      "N[NUM=sg,GEN=fem] -> 'chica' | 'mujer' | 'persona' | 'criatura'\n",
      "N[NUM=sg,GEN=masc] -> 'chico' | 'hombre' | 'sujeto' \n",
      "N[NUM=pl,GEN=fem] -> 'chicas' | 'mujeres' | 'personas' | 'criaturas'\n",
      "N[NUM=pl,GEN=masc] -> 'chicos' | 'hombres' | 'sujetos' \n",
      "# Reescritura de N vacío\n",
      "N/Wh[NUM=sg] -> \n",
      "N/Wh[NUM=pl] ->\n",
      "# Reescritura Wh\n",
      "Wh[NUM=sg] -> 'quién'\n",
      "Wh[NUM=pl] -> 'quiénes'\n",
      "# Reescritura de V\n",
      "# Verbos intransitivos\n",
      "V[SUBCAT='intrans', TENSE=pres,NUM=sg] -> 'desaparece' | 'camina' | 'muerde' | 'llora' | 'aparece' | 'viene' | 'estornuda'\n",
      "V[SUBCAT='intrans', TENSE=pres,NUM=pl] -> 'desaparecen' | 'caminan' | 'lloran' | 'muerden' | 'aparecen' | 'vienen' | 'estornudan'\n",
      "V[SUBCAT='intrans', TENSE=pas,NUM=sg] -> 'desapareció' | 'caminó' | 'mordió' | 'lloraba' | 'apareció' | 'vino' | 'estornudó'\n",
      "V[SUBCAT='intrans', TENSE=pas,NUM=pl] -> 'desaparecieron' | 'caminaron' | 'mordieron' | 'lloraban' | 'aparecieron' | 'vinieron' | 'estornudaron'\n",
      "# Verbos de decir\n",
      "V[SUBCAT='decir', TENSE=pres,NUM=sg] -> 'dice' | 'afirma' | 'defiende' | 'argumenta' | 'sostiene' \n",
      "V[SUBCAT='decir', TENSE=pas,NUM=sg] -> 'dijo' | 'afirmó' | 'defendió' | 'argumentó' | 'sostuvo' \n"
     ]
    }
   ],
   "source": [
    "nltk.data.show_cfg('gramaticas/GramaticaSlash.fcfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|.q.d.e.c.q.e.|\n",
      "Leaf Init Rule:\n",
      "|[-] . . . . .| [0:1] 'quién'\n",
      "|. [-] . . . .| [1:2] 'dice'\n",
      "|. . [-] . . .| [2:3] 'el'\n",
      "|. . . [-] . .| [3:4] 'chico'\n",
      "|. . . . [-] .| [4:5] 'que'\n",
      "|. . . . . [-]| [5:6] 'estornuda'\n",
      "Feature Empty Predict Rule:\n",
      "|# . . . . . .| [0:0] N[]/Wh[NUM='sg'] -> *\n",
      "|. # . . . . .| [1:1] N[]/Wh[NUM='sg'] -> *\n",
      "|. . # . . . .| [2:2] N[]/Wh[NUM='sg'] -> *\n",
      "|. . . # . . .| [3:3] N[]/Wh[NUM='sg'] -> *\n",
      "|. . . . # . .| [4:4] N[]/Wh[NUM='sg'] -> *\n",
      "|. . . . . # .| [5:5] N[]/Wh[NUM='sg'] -> *\n",
      "|. . . . . . #| [6:6] N[]/Wh[NUM='sg'] -> *\n",
      "|# . . . . . .| [0:0] N[]/Wh[NUM='pl'] -> *\n",
      "|. # . . . . .| [1:1] N[]/Wh[NUM='pl'] -> *\n",
      "|. . # . . . .| [2:2] N[]/Wh[NUM='pl'] -> *\n",
      "|. . . # . . .| [3:3] N[]/Wh[NUM='pl'] -> *\n",
      "|. . . . # . .| [4:4] N[]/Wh[NUM='pl'] -> *\n",
      "|. . . . . # .| [5:5] N[]/Wh[NUM='pl'] -> *\n",
      "|. . . . . . #| [6:6] N[]/Wh[NUM='pl'] -> *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[-] . . . . .| [0:1] Wh[NUM='sg'] -> 'quién' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[-> . . . . .| [0:1] S[] -> Wh[NUM=?n] * VP[]/Wh[NUM=?n] {?n: 'sg'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|. [-] . . . .| [1:2] V[NUM='sg', SUBCAT='decir', TENSE='pres'] -> 'dice' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|. [-> . . . .| [1:2] VP[NUM=?n] -> V[NUM=?n, SUBCAT='decir', TENSE=?t] * CP[] {?n: 'sg', ?t: 'pres'}\n",
      "|. [-> . . . .| [1:2] VP[]/?x[NUM=?n] -> V[NUM=?m, SUBCAT='decir', TENSE=?t] * NP[NUM=?m] CP[]/?x[NUM=?n] {?m: 'sg', ?t: 'pres'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|. . [-] . . .| [2:3] Det[GEN='masc', NUM='sg'] -> 'el' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|. . [-> . . .| [2:3] NP[GEN=?g, NUM=?n] -> Det[GEN=?g, NUM=?n] * N[GEN=?g, NUM=?n] {?g: 'masc', ?n: 'sg'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|. . . [-] . .| [3:4] N[GEN='masc', NUM='sg'] -> 'chico' *\n",
      "Feature Single Edge Fundamental Rule:\n",
      "|. . [---] . .| [2:4] NP[GEN='masc', NUM='sg'] -> Det[GEN='masc', NUM='sg'] N[GEN='masc', NUM='sg'] *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|. . [---> . .| [2:4] S[] -> NP[NUM=?n] * VP[NUM=?n] {?n: 'sg'}\n",
      "|. . [---> . .| [2:4] IP[] -> NP[NUM=?n] * VP[NUM=?n] {?n: 'sg'}\n",
      "Feature Single Edge Fundamental Rule:\n",
      "|. [-----> . .| [1:4] VP[]/?x[NUM=?n] -> V[NUM=?m, SUBCAT='decir', TENSE=?t] NP[NUM=?m] * CP[]/?x[NUM=?n] {?m: 'sg', ?t: 'pres'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|. . . . [-] .| [4:5] C[] -> 'que' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|. . . . [-> .| [4:5] CP[] -> C[] * IP[] {}\n",
      "|. . . . [-> .| [4:5] CP[]/?x[NUM=?n] -> C[] * IP[]/?x[NUM=?n] {}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|. . . . . [-]| [5:6] V[NUM='sg', SUBCAT='intrans', TENSE='pres'] -> 'estornuda' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|. . . . . [-]| [5:6] VP[NUM='sg'] -> V[NUM='sg', SUBCAT='intrans', TENSE='pres'] *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|> . . . . . .| [0:0] IP[]/?x[NUM=?n] -> N[]/?x[NUM=?n] * VP[NUM=?n] {?n: 'sg', ?x: 'Wh'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|. > . . . . .| [1:1] IP[]/?x[NUM=?n] -> N[]/?x[NUM=?n] * VP[NUM=?n] {?n: 'sg', ?x: 'Wh'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|. . > . . . .| [2:2] IP[]/?x[NUM=?n] -> N[]/?x[NUM=?n] * VP[NUM=?n] {?n: 'sg', ?x: 'Wh'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|. . . > . . .| [3:3] IP[]/?x[NUM=?n] -> N[]/?x[NUM=?n] * VP[NUM=?n] {?n: 'sg', ?x: 'Wh'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|. . . . > . .| [4:4] IP[]/?x[NUM=?n] -> N[]/?x[NUM=?n] * VP[NUM=?n] {?n: 'sg', ?x: 'Wh'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|. . . . . > .| [5:5] IP[]/?x[NUM=?n] -> N[]/?x[NUM=?n] * VP[NUM=?n] {?n: 'sg', ?x: 'Wh'}\n",
      "Feature Single Edge Fundamental Rule:\n",
      "|. . . . . [-]| [5:6] IP[]/Wh[NUM='sg'] -> N[]/Wh[NUM='sg'] VP[NUM='sg'] *\n",
      "Feature Single Edge Fundamental Rule:\n",
      "|. . . . [---]| [4:6] CP[]/Wh[NUM='sg'] -> C[] IP[]/Wh[NUM='sg'] *\n",
      "Feature Single Edge Fundamental Rule:\n",
      "|. [---------]| [1:6] VP[]/Wh[NUM='sg'] -> V[NUM='sg', SUBCAT='decir', TENSE='pres'] NP[NUM='sg'] CP[]/Wh[NUM='sg'] *\n",
      "Feature Single Edge Fundamental Rule:\n",
      "|[===========]| [0:6] S[] -> Wh[NUM='sg'] VP[]/Wh[NUM='sg'] *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|. . . . . . >| [6:6] IP[]/?x[NUM=?n] -> N[]/?x[NUM=?n] * VP[NUM=?n] {?n: 'sg', ?x: 'Wh'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|> . . . . . .| [0:0] IP[]/?x[NUM=?n] -> N[]/?x[NUM=?n] * VP[NUM=?n] {?n: 'pl', ?x: 'Wh'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|. > . . . . .| [1:1] IP[]/?x[NUM=?n] -> N[]/?x[NUM=?n] * VP[NUM=?n] {?n: 'pl', ?x: 'Wh'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|. . > . . . .| [2:2] IP[]/?x[NUM=?n] -> N[]/?x[NUM=?n] * VP[NUM=?n] {?n: 'pl', ?x: 'Wh'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|. . . > . . .| [3:3] IP[]/?x[NUM=?n] -> N[]/?x[NUM=?n] * VP[NUM=?n] {?n: 'pl', ?x: 'Wh'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|. . . . > . .| [4:4] IP[]/?x[NUM=?n] -> N[]/?x[NUM=?n] * VP[NUM=?n] {?n: 'pl', ?x: 'Wh'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|. . . . . > .| [5:5] IP[]/?x[NUM=?n] -> N[]/?x[NUM=?n] * VP[NUM=?n] {?n: 'pl', ?x: 'Wh'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|. . . . . . >| [6:6] IP[]/?x[NUM=?n] -> N[]/?x[NUM=?n] * VP[NUM=?n] {?n: 'pl', ?x: 'Wh'}\n",
      "(S[]\n",
      "  (Wh[NUM='sg'] quién)\n",
      "  (VP[]/Wh[NUM='sg']\n",
      "    (V[NUM='sg', SUBCAT='decir', TENSE='pres'] dice)\n",
      "    (NP[GEN='masc', NUM='sg']\n",
      "      (Det[GEN='masc', NUM='sg'] el)\n",
      "      (N[GEN='masc', NUM='sg'] chico))\n",
      "    (CP[]/Wh[NUM='sg']\n",
      "      (C[] que)\n",
      "      (IP[]/Wh[NUM='sg']\n",
      "        (N[]/Wh[NUM='sg'] )\n",
      "        (VP[NUM='sg']\n",
      "          (V[NUM='sg', SUBCAT='intrans', TENSE='pres'] estornuda))))))\n"
     ]
    }
   ],
   "source": [
    "sentence_slash_grammar = 'quién dice el chico que estornuda'\n",
    "sentence = sentence_slash_grammar.split()\n",
    "from nltk import load_parser\n",
    "cp = load_parser('gramaticas/GramaticaSlash.fcfg', trace=2)\n",
    "for tree in cp.parse(sentence):\n",
    "     print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
